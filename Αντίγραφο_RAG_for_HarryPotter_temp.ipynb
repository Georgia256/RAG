{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dd0d3020129c4647aff1b0f73259ca9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9c5b1f6820d418aa35ff9bbf0c2b844",
              "IPY_MODEL_8090eecc29aa429280b6523cce115b90",
              "IPY_MODEL_9780b158ad7945029bdcdd8e64567f0a"
            ],
            "layout": "IPY_MODEL_a6b82c481fd3403392a9d5349593671b"
          }
        },
        "f9c5b1f6820d418aa35ff9bbf0c2b844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d995d5306cd460e9a64446d98d5d8bf",
            "placeholder": "​",
            "style": "IPY_MODEL_4f104f524ad048ce902a9918961a889d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8090eecc29aa429280b6523cce115b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26d6b8ca110f40c080568a8412800bc9",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99aa95062dd8441f824f7b0bd7f0b2d1",
            "value": 4
          }
        },
        "9780b158ad7945029bdcdd8e64567f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6ec15874c164ece8482b73a21400872",
            "placeholder": "​",
            "style": "IPY_MODEL_5f66147094784878adb25824ff5589a1",
            "value": " 4/4 [01:19&lt;00:00, 17.04s/it]"
          }
        },
        "a6b82c481fd3403392a9d5349593671b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d995d5306cd460e9a64446d98d5d8bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f104f524ad048ce902a9918961a889d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26d6b8ca110f40c080568a8412800bc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99aa95062dd8441f824f7b0bd7f0b2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6ec15874c164ece8482b73a21400872": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f66147094784878adb25824ff5589a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "171f4304be1248609950d92f64ca6f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e96b45e9dd2475896e1943ec3a9bd86",
              "IPY_MODEL_2e53b316cbbb44e6add18cef382eee0a",
              "IPY_MODEL_4093f2e0a1224c28a173580c9675319a"
            ],
            "layout": "IPY_MODEL_b1b2e1af972742eca1e2fef4d38c1a1d"
          }
        },
        "1e96b45e9dd2475896e1943ec3a9bd86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67d30ecf28954c70b56ea4e9be6f1a5a",
            "placeholder": "​",
            "style": "IPY_MODEL_26511cf4fcba499caf69b7af567d4247",
            "value": "Parsing nodes: 100%"
          }
        },
        "2e53b316cbbb44e6add18cef382eee0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58f89953fba144d0a4138491192109b8",
            "max": 3623,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a32c27373dc4fc9895b957247c9d6f0",
            "value": 3623
          }
        },
        "4093f2e0a1224c28a173580c9675319a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91df7446a8bf40809f5cec623f08d6ed",
            "placeholder": "​",
            "style": "IPY_MODEL_6d06174aefa74e90a1c1ed20d9dbca25",
            "value": " 3623/3623 [00:07&lt;00:00, 468.98it/s]"
          }
        },
        "b1b2e1af972742eca1e2fef4d38c1a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67d30ecf28954c70b56ea4e9be6f1a5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26511cf4fcba499caf69b7af567d4247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58f89953fba144d0a4138491192109b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a32c27373dc4fc9895b957247c9d6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91df7446a8bf40809f5cec623f08d6ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d06174aefa74e90a1c1ed20d9dbca25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17b21b6b806e4f55bc1a70f3b57899a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2595089d1d9e46fdb69cfefe32676f0c",
              "IPY_MODEL_1cbd3711e58f47e2bf516ce9d49bd424",
              "IPY_MODEL_9372b8dfcfc449709d8e08df917842cf"
            ],
            "layout": "IPY_MODEL_f2b8c905b01b468a828f2e503f2a1b62"
          }
        },
        "2595089d1d9e46fdb69cfefe32676f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbe2a2eb9eef45589392e2f2ce164213",
            "placeholder": "​",
            "style": "IPY_MODEL_348a8a0c05944d3fa4b20f3e35e409de",
            "value": "Parsing nodes: 100%"
          }
        },
        "1cbd3711e58f47e2bf516ce9d49bd424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7ed927c63a949dcbe477380a51a7107",
            "max": 3623,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d76c8143e1f42df83128cedead0853a",
            "value": 3623
          }
        },
        "9372b8dfcfc449709d8e08df917842cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45022832f38d463abcb92f3d64019ba5",
            "placeholder": "​",
            "style": "IPY_MODEL_3accaede713e40579e167601bca893d3",
            "value": " 3623/3623 [00:09&lt;00:00, 472.80it/s]"
          }
        },
        "f2b8c905b01b468a828f2e503f2a1b62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbe2a2eb9eef45589392e2f2ce164213": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "348a8a0c05944d3fa4b20f3e35e409de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7ed927c63a949dcbe477380a51a7107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d76c8143e1f42df83128cedead0853a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45022832f38d463abcb92f3d64019ba5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3accaede713e40579e167601bca893d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b113ebe2e4394ba29eb0be67fa42ba6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2daf7d6d55da420d931e539cc73e0b77",
              "IPY_MODEL_277f13a458084794b91d61a777879433",
              "IPY_MODEL_67630c036be9421c84635efc427934b7"
            ],
            "layout": "IPY_MODEL_76be3aa38d8c4f809c5948727c9ded80"
          }
        },
        "2daf7d6d55da420d931e539cc73e0b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9818d808302945eb9b6391e403139938",
            "placeholder": "​",
            "style": "IPY_MODEL_8bd7f215e9cd41618d1ac755c6faac61",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "277f13a458084794b91d61a777879433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38335b09276742e080d3810296ab62ee",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f67509df9ea474c8305485c9f13bf02",
            "value": 4
          }
        },
        "67630c036be9421c84635efc427934b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8036b61d11734ff1b51ec51cc3bbfd07",
            "placeholder": "​",
            "style": "IPY_MODEL_d95643950eae49e896bd0184df649077",
            "value": " 4/4 [01:20&lt;00:00, 17.23s/it]"
          }
        },
        "76be3aa38d8c4f809c5948727c9ded80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9818d808302945eb9b6391e403139938": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bd7f215e9cd41618d1ac755c6faac61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38335b09276742e080d3810296ab62ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f67509df9ea474c8305485c9f13bf02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8036b61d11734ff1b51ec51cc3bbfd07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d95643950eae49e896bd0184df649077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N30RBvu49Mvk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6bdd193-654c-4366-af7b-bea00912c511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installs"
      ],
      "metadata": {
        "id": "IZ0R_5wF-M5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index==0.10.18 llama-index-llms-groq==0.1.3 groq==0.4.2 llama-index-embeddings-huggingface==0.2.0\n",
        "!pip install datasets==2.16.1 PyPDF2==3.0.1 langchain_experimental python-dotenv==1.0.1 pyyaml==6.0.1 coloredlogs==15.0.1 mdc==1.2.1 pytest==8.1.2\n",
        "!pip install -r drive/MyDrive/RAG-llamaindex/requirements.txt\n",
        "!pip install huggingface_hub\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip uninstall -y bitsandbytes accelerate transformers\n",
        "!pip install bitsandbytes accelerate transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tomVovEE-Mj4",
        "outputId": "8c41a3cb-be5d-4651-a47d-23af772e42a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index==0.10.18\n",
            "  Downloading llama_index-0.10.18-py3-none-any.whl (5.6 kB)\n",
            "Collecting llama-index-llms-groq==0.1.3\n",
            "  Downloading llama_index_llms_groq-0.1.3-py3-none-any.whl (2.7 kB)\n",
            "Collecting groq==0.4.2\n",
            "  Downloading groq-0.4.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-huggingface==0.2.0\n",
            "  Downloading llama_index_embeddings_huggingface-0.2.0-py3-none-any.whl (7.1 kB)\n",
            "Collecting llama-index-agent-openai<0.2.0,>=0.1.4 (from llama-index==0.10.18)\n",
            "  Downloading llama_index_agent_openai-0.1.7-py3-none-any.whl (12 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index==0.10.18)\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.18 (from llama-index==0.10.18)\n",
            "  Downloading llama_index_core-0.10.53.post1-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index==0.10.18)\n",
            "  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index==0.10.18)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index==0.10.18)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.5 (from llama-index==0.10.18)\n",
            "  Downloading llama_index_llms_openai-0.1.25-py3-none-any.whl (11 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index==0.10.18)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.7-py3-none-any.whl (5.9 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index==0.10.18)\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index==0.10.18)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index==0.10.18)\n",
            "  Downloading llama_index_readers_file-0.1.29-py3-none-any.whl (38 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index==0.10.18)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
            "Collecting llama-index-llms-openai-like<0.2.0,>=0.1.3 (from llama-index-llms-groq==0.1.3)\n",
            "  Downloading llama_index_llms_openai_like-0.1.3-py3-none-any.whl (3.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq==0.4.2) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq==0.4.2) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq==0.4.2)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq==0.4.2) (2.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq==0.4.2) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq==0.4.2) (4.12.2)\n",
            "Requirement already satisfied: huggingface-hub[inference]>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-huggingface==0.2.0) (0.23.4)\n",
            "Collecting sentence-transformers<3.0.0,>=2.6.1 (from llama-index-embeddings-huggingface==0.2.0)\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq==0.4.2) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq==0.4.2) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq==0.4.2) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq==0.4.2)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq==0.4.2)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (3.9.5)\n",
            "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0)\n",
            "  Downloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (853 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.2/853.2 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (2.0.31)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Collecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18)\n",
            "  Downloading llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18)\n",
            "  Downloading openai-1.35.10-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (9.4.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (8.4.2)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (1.14.1)\n",
            "Collecting llamaindex-py-client<0.2.0,>=0.1.19 (from llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2->llama-index==0.10.18)\n",
            "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai-like<0.2.0,>=0.1.3->llama-index-llms-groq==0.1.3) (4.41.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.18) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.18)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.18)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.18)\n",
            "  Downloading llama_parse-0.4.6-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq==0.4.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq==0.4.2) (2.20.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0) (2.3.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0) (1.11.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.18) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (2024.5.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface==0.2.0) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0) (1.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.2.0,>=0.1.3->llama-index-llms-groq==0.1.3) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.2.0,>=0.1.3->llama-index-llms-groq==0.1.3) (0.4.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.18->llama-index==0.10.18) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface==0.2.0) (1.3.0)\n",
            "Installing collected packages: striprtf, dirtyjson, pypdf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, minijinja, marshmallow, h11, deprecated, typing-inspect, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, nvidia-cusolver-cu12, httpx, dataclasses-json, openai, llamaindex-py-client, llama-cloud, groq, sentence-transformers, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-embeddings-huggingface, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-llms-openai-like, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-llms-groq, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 groq-0.4.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 llama-cloud-0.0.6 llama-index-0.10.18 llama-index-agent-openai-0.1.7 llama-index-cli-0.1.12 llama-index-core-0.10.53.post1 llama-index-embeddings-huggingface-0.2.0 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-groq-0.1.3 llama-index-llms-openai-0.1.25 llama-index-llms-openai-like-0.1.3 llama-index-multi-modal-llms-openai-0.1.7 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.29 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.6 llamaindex-py-client-0.1.19 marshmallow-3.21.3 minijinja-2.0.1 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 openai-1.35.10 pypdf-4.2.0 sentence-transformers-2.7.0 striprtf-0.0.26 tiktoken-0.7.0 typing-inspect-0.9.0\n",
            "Collecting datasets==2.16.1\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyPDF2==3.0.1\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.0.62-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.7/202.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv==1.0.1\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml==6.0.1 in /usr/local/lib/python3.10/dist-packages (6.0.1)\n",
            "Collecting coloredlogs==15.0.1\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdc==1.2.1\n",
            "  Downloading mdc-1.2.1.tar.gz (5.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytest==8.1.2\n",
            "  Downloading pytest-8.1.2-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.5/337.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.16.1)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (4.66.4)\n",
            "Collecting xxhash (from datasets==2.16.1)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets==2.16.1)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (24.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs==15.0.1)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from mdc==1.2.1) (0.18.3)\n",
            "Collecting python-json-logger (from mdc==1.2.1)\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==8.1.2) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from pytest==8.1.2) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest==8.1.2) (1.2.1)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest==8.1.2) (2.0.1)\n",
            "Collecting langchain-community<0.3.0,>=0.2.6 (from langchain_experimental)\n",
            "  Downloading langchain_community-0.2.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.3.0,>=0.2.10 (from langchain_experimental)\n",
            "  Downloading langchain_core-0.2.12-py3-none-any.whl (355 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.8/355.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets==2.16.1) (4.12.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (2.0.31)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (0.6.7)\n",
            "Collecting langchain<0.3.0,>=0.2.7 (from langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
            "  Downloading langchain-0.2.7-py3-none-any.whl (983 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
            "  Downloading langsmith-0.1.84-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.6->langchain_experimental) (8.4.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.10->langchain_experimental)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain_experimental) (2.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.16.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.16.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.16.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.16.1) (2024.6.2)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.16.1)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.1) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.1) (2024.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (0.9.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain_experimental)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community<0.3.0,>=0.2.6->langchain_experimental)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.10->langchain_experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.10->langchain_experimental) (2.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->langchain_experimental) (1.0.0)\n",
            "Building wheels for collected packages: mdc\n",
            "  Building wheel for mdc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mdc: filename=mdc-1.2.1-py3-none-any.whl size=6113 sha256=b321ac3c145a69f4407e8d35a43e6cf4d56c5a391a22de031d15166c99f3be99\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/46/20/9eff135ac0aefb36b55c046b239a0f16a5a33bb68f6e71f9d0\n",
            "Successfully built mdc\n",
            "Installing collected packages: xxhash, python-json-logger, python-dotenv, pytest, PyPDF2, orjson, jsonpointer, humanfriendly, dill, multiprocess, mdc, jsonpatch, coloredlogs, langsmith, langchain-core, datasets, langchain-text-splitters, langchain, langchain-community, langchain_experimental\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 7.4.4\n",
            "    Uninstalling pytest-7.4.4:\n",
            "      Successfully uninstalled pytest-7.4.4\n",
            "Successfully installed PyPDF2-3.0.1 coloredlogs-15.0.1 datasets-2.16.1 dill-0.3.7 humanfriendly-10.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.7 langchain-community-0.2.7 langchain-core-0.2.12 langchain-text-splitters-0.2.2 langchain_experimental-0.0.62 langsmith-0.1.84 mdc-1.2.1 multiprocess-0.70.15 orjson-3.10.6 pytest-8.1.2 python-dotenv-1.0.1 python-json-logger-2.0.7 xxhash-3.4.1\n",
            "Collecting bitsandbytes==0.43.1 (from -r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1))\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.40.2 (from -r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2))\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.29.3 (from -r drive/MyDrive/RAG-llamaindex/requirements.txt (line 3))\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (0.23.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.2->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 2)) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes==0.43.1->-r drive/MyDrive/RAG-llamaindex/requirements.txt (line 1)) (1.3.0)\n",
            "Installing collected packages: transformers, bitsandbytes, accelerate\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "Successfully installed accelerate-0.29.3 bitsandbytes-0.43.1 transformers-4.40.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.6.2)\n",
            "Collecting llama-index-llms-huggingface\n",
            "  Downloading llama_index_llms_huggingface-0.2.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.23.4)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.41 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.10.53.post1)\n",
            "Collecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface)\n",
            "  Downloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers[torch]<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.12.2)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (0.27.0)\n",
            "Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (0.0.6)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.35.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (9.4.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.14.1)\n",
            "Requirement already satisfied: pydantic<3,>2 in /usr/local/lib/python3.10/dist-packages (from text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.4.3)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.29.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (4.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.4.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.21.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.16.0)\n",
            "Installing collected packages: text-generation, llama-index-llms-huggingface\n",
            "Successfully installed llama-index-llms-huggingface-0.2.4 text-generation-0.7.0\n",
            "Found existing installation: bitsandbytes 0.43.1\n",
            "Uninstalling bitsandbytes-0.43.1:\n",
            "  Successfully uninstalled bitsandbytes-0.43.1\n",
            "Found existing installation: accelerate 0.29.3\n",
            "Uninstalling accelerate-0.29.3:\n",
            "  Successfully uninstalled accelerate-0.29.3\n",
            "Found existing installation: transformers 4.40.2\n",
            "Uninstalling transformers-4.40.2:\n",
            "  Successfully uninstalled transformers-4.40.2\n",
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Installing collected packages: transformers, bitsandbytes, accelerate\n",
            "Successfully installed accelerate-0.32.1 bitsandbytes-0.43.1 transformers-4.42.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports\n"
      ],
      "metadata": {
        "id": "7hSPbSP2_ff7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Loading the LLM\n",
        "import json\n",
        "import torch\n",
        "from transformers import (AutoTokenizer,\n",
        "                          AutoModelForCausalLM,\n",
        "                          BitsAndBytesConfig,\n",
        "                          pipeline)\n",
        "\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    ServiceContext,\n",
        "    load_index_from_storage\n",
        ")\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For Data Ingestion\n",
        "from typing import Literal, Any, List\n",
        "import logging\n",
        "import PyPDF2\n",
        "import sys\n",
        "sys.path.append('drive/MyDrive/RAG-llamaindex')\n",
        "import os\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from math import ceil\n",
        "\n",
        "# For Embeddings\n",
        "from transformers import AutoModel\n",
        "\n",
        "# For the LLM Model\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "JheyTMVNDebN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the LLM"
      ],
      "metadata": {
        "id": "QpQPXLru_YP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import (AutoTokenizer,\n",
        "                          AutoModelForCausalLM,\n",
        "                          BitsAndBytesConfig,\n",
        "                          pipeline)\n",
        "\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    ServiceContext,\n",
        "    load_index_from_storage\n",
        ")\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "EWQQMSTsDb1T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the Hugging Face token\n",
        "config_data = json.load(open(\"drive/MyDrive/RAG-llamaindex/config.json\"))\n",
        "HF_TOKEN = config_data[\"HF_TOKEN\"]\n",
        "\n",
        "# Defining the pre-trained model we will use, which is Llama-3-8B\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
        "\n",
        "# Defining the quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Defing the Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name ,\n",
        "                                          token=HF_TOKEN)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Initialising the LLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name ,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "dd0d3020129c4647aff1b0f73259ca9f",
            "f9c5b1f6820d418aa35ff9bbf0c2b844",
            "8090eecc29aa429280b6523cce115b90",
            "9780b158ad7945029bdcdd8e64567f0a",
            "a6b82c481fd3403392a9d5349593671b",
            "4d995d5306cd460e9a64446d98d5d8bf",
            "4f104f524ad048ce902a9918961a889d",
            "26d6b8ca110f40c080568a8412800bc9",
            "99aa95062dd8441f824f7b0bd7f0b2d1",
            "b6ec15874c164ece8482b73a21400872",
            "5f66147094784878adb25824ff5589a1"
          ]
        },
        "id": "v9vg2P0n_3GG",
        "outputId": "9b3c2a69-272f-45ae-ab37-97f202e913f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd0d3020129c4647aff1b0f73259ca9f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Ingestion"
      ],
      "metadata": {
        "id": "o2KK4UyYDQVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal, Any, List\n",
        "import logging\n",
        "import PyPDF2\n",
        "import sys\n",
        "sys.path.append('drive/MyDrive/RAG-llamaindex')\n",
        "import os\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from math import ceil"
      ],
      "metadata": {
        "id": "EpY7b731DYO4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = SimpleDirectoryReader(input_dir=\"drive/MyDrive/RAG-llamaindex/Data\")\n",
        "documents = reader.load_data(num_workers=4)\n",
        "\n",
        "# Filter out the data where the file_name is not \"Harry Potter The Complete Collection.pdf\"\n",
        "documents = [doc for doc in documents if doc.metadata[\"file_name\"] == \"Harry Potter The Complete Collection.pdf\"]\n",
        "\n",
        "# Creating a function for chunking using the LLama Index SentenceSplitter\n",
        "def get_chunks(\n",
        "    documents: list[Any],\n",
        "    chunk_size: int = 512,\n",
        "    chunk_overlap: float = 0.0\n",
        ") -> list[str]:\n",
        "    overlap = int(chunk_size * chunk_overlap)\n",
        "    text_splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
        "    chunks= text_splitter.get_nodes_from_documents(documents, show_progress=True)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Creating a function for manual chunking\n",
        "def get_chunks_manual(documents: list[Any], chunk_size: int = 512, chunk_overlap: float = 0.0) -> List[str]:\n",
        "    \"\"\"\n",
        "    Takes in a `file_path`, retrieves the document, breaks it down into chunks of size\n",
        "    `chunk_size` and overlap `chunk_overlap`, and returns the chunks.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    text = \"\"\n",
        "    for doc in documents:\n",
        "        text += doc.text\n",
        "    # replace every \\t with \" \"\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "\n",
        "    step_size = ceil(chunk_size * (1 - chunk_overlap))\n",
        "    for i in range(0, len(text), step_size):\n",
        "        chunks.append(text[i:i + chunk_size])\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "apM3xQAvDWGA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the Sentence Splitter\n",
        "\n",
        "# Creating chunks of 512 characters with no overlap\n",
        "chunks_512_0 = get_chunks(documents)\n",
        "print(\"Created\", len(chunks_512_0), \"chunks of size 512 and 0% overlap with the SentenceSplitter\")\n",
        "print(\"The 100th chunk is:\", chunks_512_0[100])\n",
        "\n",
        "# Creating chunks of 512 with 50% overlap\n",
        "chunks_512_50 = get_chunks(documents, chunk_size=512, chunk_overlap=0.5)\n",
        "print(\"Created\", len(chunks_512_50), \"chunks of size 512 and 0% overlap with the SentenceSplitter\")\n",
        "print(\"The 100th chunk is:\", chunks_512_50[100])"
      ],
      "metadata": {
        "id": "FDJTFtmxDuxZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473,
          "referenced_widgets": [
            "171f4304be1248609950d92f64ca6f0a",
            "1e96b45e9dd2475896e1943ec3a9bd86",
            "2e53b316cbbb44e6add18cef382eee0a",
            "4093f2e0a1224c28a173580c9675319a",
            "b1b2e1af972742eca1e2fef4d38c1a1d",
            "67d30ecf28954c70b56ea4e9be6f1a5a",
            "26511cf4fcba499caf69b7af567d4247",
            "58f89953fba144d0a4138491192109b8",
            "4a32c27373dc4fc9895b957247c9d6f0",
            "91df7446a8bf40809f5cec623f08d6ed",
            "6d06174aefa74e90a1c1ed20d9dbca25",
            "17b21b6b806e4f55bc1a70f3b57899a1",
            "2595089d1d9e46fdb69cfefe32676f0c",
            "1cbd3711e58f47e2bf516ce9d49bd424",
            "9372b8dfcfc449709d8e08df917842cf",
            "f2b8c905b01b468a828f2e503f2a1b62",
            "cbe2a2eb9eef45589392e2f2ce164213",
            "348a8a0c05944d3fa4b20f3e35e409de",
            "b7ed927c63a949dcbe477380a51a7107",
            "0d76c8143e1f42df83128cedead0853a",
            "45022832f38d463abcb92f3d64019ba5",
            "3accaede713e40579e167601bca893d3"
          ]
        },
        "outputId": "a10aa00a-ca41-4de2-82ea-09faca1e86e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/3623 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "171f4304be1248609950d92f64ca6f0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 6882 chunks of size 512 and 0% overlap with the SentenceSplitter\n",
            "The 100th chunk is: Node ID: 6f915fbc-29db-474e-9b30-6cceb653ecbd\n",
            "Text: about     you,    probably        nothing a       good\n",
            "beating wouldn’t        have    cured   —       and     as for     all\n",
            "this    about   your    parents,        well,   they    were\n",
            "weirdos,        no      denying it,     and     the world’s better\n",
            "off     without them    in      my      opinion —       asked   for\n",
            "all     they    got, getting mixed   up      with    these   wizarding\n",
            "types   —       just    what    I       expected,       always knew\n",
            "they’d  come    to      a       sticky  end     —” But     at\n",
            "that...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/3623 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17b21b6b806e4f55bc1a70f3b57899a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 8170 chunks of size 512 and 0% overlap with the SentenceSplitter\n",
            "The 100th chunk is: Node ID: 36f0b8ed-9aeb-4d9a-b15e-23185e45e2ac\n",
            "Text: He        lay     and     watched his     birthday        tick\n",
            "nearer, wondering       if      the     Dursleys        would\n",
            "remember        at      all,    wondering       where   the     letter\n",
            "writer  was     now. Five    minutes to      go.     Harry   heard\n",
            "something       creak   outside.        He      hoped   the roof\n",
            "wasn’t  going   to      fall    in,     although        he      might\n",
            "be      warmer  if      it      did.    Four minutes to      go.\n",
            "Maybe   the     house   in      Privet  Drive   would   be      so\n",
            "full    of...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the Manual Chunking\n",
        "\n",
        "# Creating chunks of 512 with no overlap\n",
        "manual_chunks_512_0 = get_chunks_manual(documents, 512, 0.0)\n",
        "print(\"Created\", len(manual_chunks_512_0), \"chunks of size 512 and 0% overlap with the Manual Splitter\")\n",
        "print(\"The 100th chunk is:\", manual_chunks_512_0[100])\n",
        "\n",
        "# Creating chunks of 512 with 50% overlap\n",
        "manual_chunks_512_50 = get_chunks_manual(documents, 512, 0.5)\n",
        "print(\"Created\", len(manual_chunks_512_50), \"chunks of size 512 and 50% overlap with the Manual Splitter\")\n",
        "print(\"The 100th chunk is:\", manual_chunks_512_50[100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVAev7dKESL5",
        "outputId": "ca850424-699a-468e-cb79-6e9a376d2132"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 12269 chunks of size 512 and 0% overlap with the Manual Splitter\n",
            "The 100th chunk is: en the bill, snorted in disgust, and flipped over the\n",
            "postcard.\n",
            "“Marge’s ill,” he informed Aunt Petunia. “Ate a funny whelk . . .”\n",
            "“Dad!” said Dudley suddenly. “Dad, Harry’s got something!”\n",
            "Harry was on the point of unfolding his letter, which was written on the\n",
            "same heavy parchment as the envelope, when it was jerked sharply out of his\n",
            "hand by Uncle Vernon.\n",
            "“That’s \n",
            "mine\n",
            "!” said Harry, trying to snatch it back.\n",
            "“Who’d be writing to you?” sneered Uncle Vernon, shaking the letter open\n",
            "with one hand and glanc\n",
            "Created 24537 chunks of size 512 and 50% overlap with the Manual Splitter\n",
            "The 100th chunk is: and took out the silver Put-Outer. He clicked it once, and twelve balls\n",
            "of light sped back to their street lamps so that Privet Drive glowed suddenly\n",
            "orange and he could make out a tabby cat slinking around the corner at the\n",
            "other end of the street. He could just see the bundle of blankets on the step of\n",
            "number four.\n",
            "“Good luck, Harry,” he murmured. He turned on his heel and with a swish\n",
            "of his cloak, he was gone.\n",
            "A breeze ruffled the neat hedges of Privet Drive, which lay silent and tidy\n",
            "under the inky sky\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating the Embedding Model"
      ],
      "metadata": {
        "id": "TH2SdVadE1rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel"
      ],
      "metadata": {
        "id": "WQbDHvVgE52H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HuggingFaceEmbedding:\n",
        "    def __init__(self, model_name: str):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def get_text_embedding(self, text: str) -> torch.Tensor:\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
        "\n",
        "    def get_embeddings(self, texts: list[str], batch_size: int = 16) -> list[torch.Tensor]:\n",
        "        embeddings = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            inputs = self.tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "            embeddings.extend(batch_embeddings.cpu())\n",
        "        return embeddings\n",
        "\n",
        "embed_model =  HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n"
      ],
      "metadata": {
        "id": "ba5JlmwiE9jt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining the LLM Model"
      ],
      "metadata": {
        "id": "h0T-_rXtFZYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "wEugdMqWFQFM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm= HuggingFaceLLM(model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Find the embedding of each text in Manual_Chunks of size 512 and 0% overlap and put in list\n",
        "manual_512_0_embed = embed_model.get_embeddings(manual_chunks_512_0)\n",
        "manual_512_0_embed_np = [embedding.numpy() for embedding in manual_512_0_embed]\n",
        "print(len(manual_512_0_embed_np))\n",
        "\n",
        "# Find the embedding of each text in Manual_Chunks of size 512 and 50% overlap and put in list\n",
        "manual_512_50_embed = embed_model.get_embeddings(manual_chunks_512_50)\n",
        "manual_512_50_embed_np = [embedding.numpy() for embedding in manual_512_50_embed]\n",
        "print(len(manual_512_50_embed_np))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWzHfPizH0Wf",
        "outputId": "be1b38e0-1fdf-4865-c3e6-249bed8fb5b5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.llms.huggingface.base:The model `StabilityAI/stablelm-tuned-alpha-3b` and tokenizer `meta-llama/Meta-Llama-3-8B` are different, please ensure that they are compatible.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12269\n",
            "24537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the first Retriever: Top-k Embeddings Retriever\n",
        "\n",
        "def top_k_embed_retriever(query: str, k: int, embed_model, manual_embed_np, texts):\n",
        "    query_embed = embed_model.get_text_embedding(query)\n",
        "    query_embed_np = query_embed.cpu().numpy().reshape(1, -1)\n",
        "\n",
        "    # Calculate cosine similarity with all the other embeddings\n",
        "    similarities = cosine_similarity(query_embed_np, manual_embed_np).flatten()\n",
        "\n",
        "    # Find the top k indices\n",
        "    top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
        "    top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]\n",
        "\n",
        "    # Retrieve the top k texts\n",
        "    top_k_texts = [texts[i] for i in top_k_indices]\n",
        "\n",
        "    return top_k_texts\n",
        "\"\"\"\n",
        "# Example usage\n",
        "query = \"Hogwarts\"\n",
        "k = 3\n",
        "top_k_texts = top_k_embed_retriever(query, k, embed_model, np.stack(manual_embed_np), manual_chunks)\n",
        "print(top_k_texts[0])\n",
        "print(\"___________\")\n",
        "print(top_k_texts[1])\n",
        "print(\"___________\")\n",
        "print(top_k_texts[2])\"\"\""
      ],
      "metadata": {
        "id": "S11Ruw8jIIMD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "66cbb1a1-bb6b-4ffc-f7e2-622b0d0331f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Example usage\\nquery = \"Hogwarts\"\\nk = 3\\ntop_k_texts = top_k_embed_retriever(query, k, embed_model, np.stack(manual_embed_np), manual_chunks)\\nprint(top_k_texts[0])\\nprint(\"___________\")\\nprint(top_k_texts[1])\\nprint(\"___________\")\\nprint(top_k_texts[2])'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_512_0 = TfidfVectorizer()\n",
        "vectorizer_512_50 = TfidfVectorizer()\n",
        "\n",
        "tfidf_matrix_512_0 = vectorizer_512_0.fit_transform(manual_chunks_512_0)\n",
        "tfidf_matrix_512_50 = vectorizer_512_50.fit_transform(manual_chunks_512_50)"
      ],
      "metadata": {
        "id": "snrnhO_NJJnA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the second Retriever: Top-k TF-IDF Retriever\n",
        "def top_k_tfidf_retriever(query: str, k: int, tfidf_matrix, texts, vectorizer):\n",
        "    query_vector = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "    top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
        "    top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]\n",
        "    top_k_texts = [texts[i] for i in top_k_indices]\n",
        "    return top_k_texts\n",
        "\n",
        "\"\"\"# Example usage\n",
        "query = \"Hogwarts\"\n",
        "k = 3\n",
        "top_k_texts = top_k_tfidf_retriever(query, k, tfidf_matrix_512_0, manual_chunks, vectorizer_512_0)\n",
        "print(top_k_texts[0])\n",
        "print(\"___________\")\n",
        "print(top_k_texts[1])\n",
        "print(\"___________\")\n",
        "print(top_k_texts[2])\"\"\""
      ],
      "metadata": {
        "id": "Jt_wBz1NJWuE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "28afae46-a8dd-4401-f206-b34f43dbdc82"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Example usage\\nquery = \"Hogwarts\"\\nk = 3\\ntop_k_texts = top_k_tfidf_retriever(query, k, tfidf_matrix_512_0, manual_chunks, vectorizer_512_0)\\nprint(top_k_texts[0])\\nprint(\"___________\")\\nprint(top_k_texts[1])\\nprint(\"___________\")\\nprint(top_k_texts[2])'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating the Pipeline"
      ],
      "metadata": {
        "id": "J9PichlZMMT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "hPbdl0scMOWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HuggingFaceLLM:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def generate_answer(self, prompt: str) -> str:\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "        outputs = self.model.generate(**inputs)\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "llm = HuggingFaceLLM(model=model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"what's 2+2?. Answer should be only one number.\"\n",
        "llm_answer = llm.generate_answer(prompt)\n",
        "print(llm_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GR-rkKOhpRO",
        "outputId": "b11f3170-c4a6-4c60-dd14-3ac9af02eda6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what's 2+2?. Answer should be only one number. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89. 90. 91. 92. 93. 94. 95. 96. 97. 98. 99. 100.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Getting the Hugging Face token\n",
        "config_data = json.load(open(\"drive/MyDrive/RAG-llamaindex/config.json\"))\n",
        "HF_TOKEN = config_data[\"HF_TOKEN\"]\n",
        "\n",
        "# Defining the pre-trained model we will use, which is Llama-3-8B\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
        "\n",
        "# Defining the quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Defing the Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name ,\n",
        "                                          token=HF_TOKEN)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Initialising the LLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name ,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "# Function to generate a response\n",
        "def generate_response(question):\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs['input_ids'], max_length=100, num_return_sequences=1)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QAjpgwZSmEOI",
        "outputId": "69e81c3a-afaf-4901-bc8a-3e972afa89e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "b113ebe2e4394ba29eb0be67fa42ba6e",
            "2daf7d6d55da420d931e539cc73e0b77",
            "277f13a458084794b91d61a777879433",
            "67630c036be9421c84635efc427934b7",
            "76be3aa38d8c4f809c5948727c9ded80",
            "9818d808302945eb9b6391e403139938",
            "8bd7f215e9cd41618d1ac755c6faac61",
            "38335b09276742e080d3810296ab62ee",
            "7f67509df9ea474c8305485c9f13bf02",
            "8036b61d11734ff1b51ec51cc3bbfd07",
            "d95643950eae49e896bd0184df649077"
          ]
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b113ebe2e4394ba29eb0be67fa42ba6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the capital of France?\n",
            "Answer: What is the capital of France? Paris, of course. And what is the capital of the US? Washington, D.C., of course. But what is the capital of the world? That’s a question that many people have been trying to answer for centuries. And while there is no one definitive answer, there are some contenders that have emerged over the years.\n",
            "What is the capital of the world?\n",
            "There is no definitive answer to this question, as the capital of the world is subjective.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(question):\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=1000,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,  # To reduce repetition\n",
        "            pad_token_id=tokenizer.eos_token_id  # Set pad token id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.strip()"
      ],
      "metadata": {
        "id": "NE8FGhgtoeMS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is 2+2? Answer: 4. \\nWhat is the capital of France? Answer: Paris. \\n What's 5 times 7? Answer:\"\n",
        "answer = generate_response(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "OydntvDAn86I",
        "outputId": "e7e278f8-13e0-4d65-f554-926d63d3677e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is 2+2? Answer: 4. \n",
            "What is the capital of France? Answer: Paris. \n",
            " What's 5 times 7? Answer:\n",
            "Answer: What is 2+2? Answer: 4. \n",
            "What is the capital of France? Answer: Paris. \n",
            " What's 5 times 7? Answer: Thirty-five.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_pipeline(documents: list[Any], df: pd.DataFrame, chunk_sz: int = 512, chunk_ol: float = 0.0, retriever: str = \"embed\", k: int = 3, embd_mod=None):\n",
        "    '''\n",
        "    This function takes a dataframe `df` that contains the QA's, `chunk_sz` and overlap `chunk_ol`,\n",
        "     a retriver `retiever` between the top_k_embed_retriever and the top_k_tfidf_retriever,\n",
        "    the parameter `k` for the retrievers and an embeddings model.\n",
        "    '''\n",
        "\n",
        "    # Creating the chunks\n",
        "    chunks = get_chunks_manual(documents, chunk_sz, chunk_ol)\n",
        "\n",
        "    # Manual Embeddings for TF-IDF\n",
        "    if retriever == 'embed':\n",
        "      # Loading the embedding model and embed the chunks\n",
        "      manual_embed = embed_model.get_embeddings(chunks)\n",
        "      manual_embed_np = [embedding.numpy() for embedding in manual_embed]\n",
        "    else:\n",
        "      # Calculating TF-IDF\n",
        "      vectorizer = TfidfVectorizer()\n",
        "      tfidf_matrix = vectorizer.fit_transform(chunks)\n",
        "\n",
        "    results = []\n",
        "    for index, row in df.iterrows():\n",
        "      query = row[\"Question\"]\n",
        "\n",
        "      if retriever == \"embed\":\n",
        "        top_k_texts = top_k_embed_retriever(query, k, embed_model, np.stack(manual_embed_np), chunks)\n",
        "      else:\n",
        "        top_k_texts = top_k_tfidf_retriever(query, k, tfidf_matrix, chunks, vectorizer)\n",
        "\n",
        "      retrieved_context = \"\\n\\n\".join(top_k_texts)\n",
        "\n",
        "      instruction1 = \"Q: What is the spell to open doors? \\n Context: she shouted Alohomora and the door opened\\n Answer: Alohomora.\\n\"\n",
        "      instruction2 = \"Q: What is the name of Harry's mom? \\n Context: he saw the picture of his parents: James and Lily\\n Answer: Lily.\\n\"\n",
        "\n",
        "      prompt = f\"{instruction1}{instruction2}Q: {query}\\nContext: {retrieved_context}\\nAnswer:\"\n",
        "      llm_answer = generate_response(prompt)\n",
        "\n",
        "      result = {\n",
        "          \"Question\": query,\n",
        "          \"True_Answer\": row['Answer'],\n",
        "           \"Retrieved_Context\": retrieved_context,\n",
        "           \"LLM_Answer\": llm_answer\n",
        "      }\n",
        "      results.append(result)\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "KVIJedGRqiKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def rag_pipeline(documents: list[Any], df: pd.DataFrame, chunk_sz: int = 512, chunk_ol: float = 0.0, retriever: str = \"embed\", k: int = 3, embd_mod=None):\n",
        "#     '''\n",
        "#     This function takes a dataframe `df` that contains the QA's, `chunk_sz` and overlap `chunk_ol`,\n",
        "#      a retriver `retiever` between the top_k_embed_retriever and the top_k_tfidf_retriever,\n",
        "#     the parameter `k` for the retrievers and an embeddings model.\n",
        "#     '''\n",
        "\n",
        "#     # Creating the chunks\n",
        "#     chunks = get_chunks_manual(documents, chunk_sz, chunk_ol)\n",
        "\n",
        "#     # Manual Embeddings for TF-IDF\n",
        "#     if retriever == 'embed':\n",
        "#       # Loading the embedding model and embed the chunks\n",
        "#       manual_embed = embed_model.get_embeddings(chunks)\n",
        "#       manual_embed_np = [embedding.numpy() for embedding in manual_embed]\n",
        "#     else:\n",
        "#       # Calculating TF-IDF\n",
        "#       vectorizer = TfidfVectorizer()\n",
        "#       tfidf_matrix = vectorizer.fit_transform(chunks)\n",
        "\n",
        "#     results = []\n",
        "#     for index, row in df.iterrows():\n",
        "#       query = row[\"Question\"]\n",
        "\n",
        "#       if retriever == \"embed\":\n",
        "#         top_k_texts = top_k_embed_retriever(query, k, embed_model, np.stack(manual_embed_np), chunks)\n",
        "#       else:\n",
        "#         top_k_texts = top_k_tfidf_retriever(query, k, tfidf_matrix, chunks, vectorizer)\n",
        "\n",
        "#       retrieved_context = \" \".join(top_k_texts)\n",
        "#       prompt = f\"Q: {query}\\nContext: {retrieved_context}\\nAnswer:\"\n",
        "#       llm_answer = llm.generate_answer(prompt)\n",
        "\n",
        "#       result = {\n",
        "#           \"Question\": query,\n",
        "#           \"True_Answer\": row['Answer'],\n",
        "#            \"Retrieved_Context\": retrieved_context,\n",
        "#            \"LLM_Answer\": llm_answer\n",
        "#       }\n",
        "#       results.append(result)\n",
        "\n",
        "#     return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "KnDWFMCsMqwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [doc for doc in documents if doc.metadata[\"file_name\"] == \"Harry Potter The Complete Collection.pdf\"]\n",
        "import csv\n",
        "harrypotter = documents[0]\n",
        "\n",
        "class HuggingFaceLLM:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def generate_answer(self, prompt: str) -> str:\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "        outputs = self.model.generate(**inputs)\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "llm = HuggingFaceLLM(model=model, tokenizer=tokenizer)\n",
        "embed_model =  HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "df = pd.read_csv('drive/MyDrive/RAG-llamaindex/harry-potter-data.csv')\n",
        "\n",
        "\"\"\"result_df = rag_pipeline(harrypotter, df, chunk_sz=512, chunk_ol=0.0, retriever=\"embed\", k=3, embd_mod=embed_model)\n",
        "result_df.to_csv('retrieved_answers.csv', index=False)\n",
        "print(result_df.head())\"\"\"\n",
        "\n",
        "with open('drive/MyDrive/RAG-llamaindex/results.csv', 'w', newline='') as csvfile:\n",
        "    fieldnames = ['Question', 'Context', 'Answer']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Process each question and write the result immediately\n",
        "    for q in df['Question']:\n",
        "        result = rag_pipeline(harrypotter, q, chunk_size=512, chunk_overlap=0.0, retriever=\"embed\", k=3)\n",
        "        writer.writerow(result)\n",
        "\n",
        "print(\"Results saved successfully.\")"
      ],
      "metadata": {
        "id": "A-5QlJ5aM6ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GmL8QP2eRbP7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}