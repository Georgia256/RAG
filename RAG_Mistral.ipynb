{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qs_amRC-rdGL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ba608f52d40c484c9a2b2f85205ac481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9bc7a991972443f885eab29d8544e949",
              "IPY_MODEL_c3f9667c49c9431d9569ff79defdcd21",
              "IPY_MODEL_491c0e45cab64151a74510cc84a5995b"
            ],
            "layout": "IPY_MODEL_e2ca414392164e2c8ca8896aefafe451"
          }
        },
        "9bc7a991972443f885eab29d8544e949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2997bb41a23f4a3bb51e53f02892414a",
            "placeholder": "​",
            "style": "IPY_MODEL_04beb3252a714a87b0a05b96ad4c858f",
            "value": "Parsing nodes: 100%"
          }
        },
        "c3f9667c49c9431d9569ff79defdcd21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_219a445666ba459bb941ea48131f47d4",
            "max": 3623,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2df28fcb27e40a299a7516a5dd9596d",
            "value": 3623
          }
        },
        "491c0e45cab64151a74510cc84a5995b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1ce7c92a31d498da9cda110e5f1d666",
            "placeholder": "​",
            "style": "IPY_MODEL_dbf00f7a28dd4ea8a3d4e74765ecd4dc",
            "value": " 3623/3623 [00:04&lt;00:00, 963.98it/s]"
          }
        },
        "e2ca414392164e2c8ca8896aefafe451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2997bb41a23f4a3bb51e53f02892414a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04beb3252a714a87b0a05b96ad4c858f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "219a445666ba459bb941ea48131f47d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2df28fcb27e40a299a7516a5dd9596d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1ce7c92a31d498da9cda110e5f1d666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbf00f7a28dd4ea8a3d4e74765ecd4dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcYZ_CLNQjKg",
        "outputId": "26e25b97-40d7-4300-b4a9-144f7aaa4472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "llama-index\n",
        "llama-index-llms-huggingface\n",
        "llama-index-embeddings-huggingface\n",
        "chromadb\n",
        "llama-index-vector-stores-chroma\n",
        "llama-index-llms-groq\n",
        "einops\n",
        "accelerate\n",
        "sentence-transformers\n",
        "llama-index-llms-mistralai\n",
        "llama-index-embeddings-mistralai\n",
        "chainlit\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TL6x5KJFSEVI",
        "outputId": "60bff223-f603-4c18-fa1a-4070f39f7393"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index (from -r requirements.txt (line 1))\n",
            "  Downloading llama_index-0.10.53-py3-none-any.whl (6.8 kB)\n",
            "Collecting llama-index-llms-huggingface (from -r requirements.txt (line 2))\n",
            "  Downloading llama_index_llms_huggingface-0.2.4-py3-none-any.whl (11 kB)\n",
            "Collecting llama-index-embeddings-huggingface (from -r requirements.txt (line 3))\n",
            "  Downloading llama_index_embeddings_huggingface-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting chromadb (from -r requirements.txt (line 4))\n",
            "  Downloading chromadb-0.5.3-py3-none-any.whl (559 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-vector-stores-chroma (from -r requirements.txt (line 5))\n",
            "  Downloading llama_index_vector_stores_chroma-0.1.10-py3-none-any.whl (5.0 kB)\n",
            "Collecting llama-index-llms-groq (from -r requirements.txt (line 6))\n",
            "  Downloading llama_index_llms_groq-0.1.4-py3-none-any.whl (2.9 kB)\n",
            "Collecting einops (from -r requirements.txt (line 7))\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate (from -r requirements.txt (line 8))\n",
            "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers (from -r requirements.txt (line 9))\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-mistralai (from -r requirements.txt (line 10))\n",
            "  Downloading llama_index_llms_mistralai-0.1.17-py3-none-any.whl (5.6 kB)\n",
            "Collecting llama-index-embeddings-mistralai (from -r requirements.txt (line 11))\n",
            "  Downloading llama_index_embeddings_mistralai-0.1.4-py3-none-any.whl (2.6 kB)\n",
            "Collecting chainlit (from -r requirements.txt (line 12))\n",
            "  Downloading chainlit-1.1.306-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_agent_openai-0.2.8-py3-none-any.whl (13 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core==0.10.53.post1 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_core-0.10.53.post1-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.2.3-py3-none-any.whl (9.2 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_llms_openai-0.1.25-py3-none-any.whl (11 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.7-py3-none-any.whl (5.9 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_readers_file-0.1.29-py3-none-any.whl (38 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (3.9.5)\n",
            "Collecting dataclasses-json (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading openai-1.35.10-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (8.4.2)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface->-r requirements.txt (line 2)) (0.23.4)\n",
            "Collecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Downloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface->-r requirements.txt (line 2)) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers[torch]<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface->-r requirements.txt (line 2)) (4.41.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 4)) (1.2.1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 4)) (2.8.0)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 4)) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 4)) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 4)) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 4)) (0.12.3)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmh3>=4.0.1 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.9.12 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai-like<0.2.0,>=0.1.3 (from llama-index-llms-groq->-r requirements.txt (line 6))\n",
            "  Downloading llama_index_llms_openai_like-0.1.3-py3-none-any.whl (3.0 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 8)) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 8)) (0.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (1.11.4)\n",
            "Collecting mistralai>=0.4.0 (from llama-index-llms-mistralai->-r requirements.txt (line 10))\n",
            "  Downloading mistralai-0.4.2-py3-none-any.whl (20 kB)\n",
            "Collecting aiofiles<24.0.0,>=23.1.0 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting asyncer<0.0.3,>=0.0.2 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading asyncer-0.0.2-py3-none-any.whl (8.3 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from chainlit->-r requirements.txt (line 12)) (8.1.7)\n",
            "Collecting dataclasses-json (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting fastapi>=0.95.2 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filetype<2.0.0,>=1.2.0 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting lazify<0.5.0,>=0.4.0 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading Lazify-0.4.0-py2.py3-none-any.whl (3.1 kB)\n",
            "Collecting literalai==0.0.607 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading literalai-0.0.607.tar.gz (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy<2.0.0 (from llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=20.0 (from accelerate->-r requirements.txt (line 8))\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjwt<3.0.0,>=2.8.0 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.0 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting python-multipart<0.0.10,>=0.0.9 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting python-socketio<6.0.0,>=5.11.0 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading python_socketio-5.11.3-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.38.0,>=0.37.2 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting syncer<3.0.0,>=2.0.3 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading syncer-2.0.3.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from chainlit->-r requirements.txt (line 12)) (2.0.1)\n",
            "Collecting uptrace<2.0.0,>=1.22.0 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading uptrace-1.24.0-py3-none-any.whl (8.6 kB)\n",
            "Collecting uvicorn<0.26.0,>=0.25.0 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles<0.21.0,>=0.20.0 (from chainlit->-r requirements.txt (line 12))\n",
            "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chevron>=0.14.0 (from literalai==0.0.607->chainlit->-r requirements.txt (line 12))\n",
            "  Downloading chevron-0.14.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from asyncer<0.0.3,>=0.0.2->chainlit->-r requirements.txt (line 12)) (3.7.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 4)) (1.1.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface->-r requirements.txt (line 2)) (3.15.4)\n",
            "Collecting minijinja>=1.0 (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Downloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (853 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.2/853.2 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index->-r requirements.txt (line 1)) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_parse-0.4.6-py3-none-any.whl (9.1 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (1.12.1)\n",
            "Collecting importlib-metadata<=7.1,>=6.0 (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 4)) (1.63.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.25.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.25.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading opentelemetry_proto-1.25.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 4)) (67.7.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb->-r requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb->-r requirements.txt (line 4)) (2.20.0)\n",
            "Requirement already satisfied: bidict>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio<6.0.0,>=5.11.0->chainlit->-r requirements.txt (line 12)) (0.23.1)\n",
            "Collecting python-engineio>=4.8.0 (from python-socketio<6.0.0,>=5.11.0->chainlit->-r requirements.txt (line 12))\n",
            "  Downloading python_engineio-4.9.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface->-r requirements.txt (line 2)) (2024.5.15)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 4)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 4)) (13.7.1)\n",
            "Collecting opentelemetry-exporter-otlp~=1.24 (from uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 12))\n",
            "  Downloading opentelemetry_exporter_otlp-1.25.0-py3-none-any.whl (7.0 kB)\n",
            "INFO: pip is looking at multiple versions of uvicorn[standard] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting uvicorn[standard]>=0.18.3 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading uvicorn-0.30.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading uvicorn-0.28.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading uvicorn-0.28.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading uvicorn-0.27.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of uvicorn[standard] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading uvicorn-0.26.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn<0.26.0,>=0.25.0->chainlit->-r requirements.txt (line 12))\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn<0.26.0,>=0.25.0->chainlit->-r requirements.txt (line 12))\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn<0.26.0,>=0.25.0->chainlit->-r requirements.txt (line 12))\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 9)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 9)) (3.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.4.0->asyncer<0.0.3,>=0.0.2->chainlit->-r requirements.txt (line 12)) (1.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index->-r requirements.txt (line 1)) (2.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 4)) (3.19.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (1.7.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.25.0 (from opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit->-r requirements.txt (line 12))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.25.0-py3-none-any.whl (16 kB)\n",
            "Collecting simple-websocket>=0.10.0 (from python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit->-r requirements.txt (line 12))\n",
            "  Downloading simple_websocket-1.0.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb->-r requirements.txt (line 4)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb->-r requirements.txt (line 4)) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.53.post1->llama-index->-r requirements.txt (line 1)) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb->-r requirements.txt (line 4)) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (0.6.0)\n",
            "Collecting wsproto (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit->-r requirements.txt (line 12))\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: literalai, pypika, syncer\n",
            "  Building wheel for literalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for literalai: filename=literalai-0.0.607-py3-none-any.whl size=64143 sha256=3c54d413b71367a0a91b12f0ce8d20f963707fb5e1032853fff157c59c08cdf2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/e9/96/826c366cb47a2d2b23265f59beb18f9d5635ab75185cff7a9e\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=758446eb0390cb963e93678cbcd17da549c41d6978ae14e3cc100a0c03d6a447\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "  Building wheel for syncer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for syncer: filename=syncer-2.0.3-py2.py3-none-any.whl size=3436 sha256=ca10ff8079ab1cc9e2a94b75ad677bca5c5fd1f70f323b4e51a78431396a11df\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/e4/36/bcaad665bcc2b672888ff2df1a5a1dc638378d8765055313cd\n",
            "Successfully built literalai pypika syncer\n",
            "Installing collected packages: syncer, striprtf, pypika, monotonic, mmh3, lazify, filetype, dirtyjson, chevron, websockets, uvloop, python-multipart, python-dotenv, pypdf, pyjwt, packaging, overrides, orjson, opentelemetry-util-http, opentelemetry-proto, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, mypy-extensions, minijinja, importlib-metadata, humanfriendly, httptools, h11, einops, deprecated, bcrypt, backoff, asgiref, aiofiles, wsproto, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, marshmallow, httpcore, coloredlogs, chroma-hnswlib, asyncer, text-generation, simple-websocket, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, nvidia-cusolver-cu12, kubernetes, httpx, fastapi, dataclasses-json, python-engineio, opentelemetry-sdk, opentelemetry-instrumentation-asgi, openai, mistralai, llama-cloud, literalai, sentence-transformers, python-socketio, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, llama-index-legacy, llama-index-core, accelerate, opentelemetry-exporter-otlp, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-llms-mistralai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-embeddings-mistralai, llama-index-embeddings-huggingface, chromadb, uptrace, llama-index-vector-stores-chroma, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-llms-openai-like, llama-index-llms-huggingface, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-llms-groq, chainlit, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: pyjwt\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.0.0\n",
            "    Uninstalling importlib_metadata-8.0.0:\n",
            "      Successfully uninstalled importlib_metadata-8.0.0\n",
            "Successfully installed accelerate-0.32.1 aiofiles-23.2.1 asgiref-3.8.1 asyncer-0.0.2 backoff-2.2.1 bcrypt-4.1.3 chainlit-1.1.306 chevron-0.14.0 chroma-hnswlib-0.7.3 chromadb-0.5.3 coloredlogs-15.0.1 dataclasses-json-0.5.14 deprecated-1.2.14 dirtyjson-1.0.8 einops-0.8.0 fastapi-0.110.3 filetype-1.2.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 humanfriendly-10.0 importlib-metadata-7.1.0 kubernetes-30.1.0 lazify-0.4.0 literalai-0.0.607 llama-cloud-0.0.6 llama-index-0.10.53 llama-index-agent-openai-0.2.8 llama-index-cli-0.1.12 llama-index-core-0.10.53.post1 llama-index-embeddings-huggingface-0.2.2 llama-index-embeddings-mistralai-0.1.4 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.2.3 llama-index-legacy-0.9.48 llama-index-llms-groq-0.1.4 llama-index-llms-huggingface-0.2.4 llama-index-llms-mistralai-0.1.17 llama-index-llms-openai-0.1.25 llama-index-llms-openai-like-0.1.3 llama-index-multi-modal-llms-openai-0.1.7 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.29 llama-index-readers-llama-parse-0.1.6 llama-index-vector-stores-chroma-0.1.10 llama-parse-0.4.6 marshmallow-3.21.3 minijinja-2.0.1 mistralai-0.4.2 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 onnxruntime-1.18.1 openai-1.35.10 opentelemetry-api-1.25.0 opentelemetry-exporter-otlp-1.25.0 opentelemetry-exporter-otlp-proto-common-1.25.0 opentelemetry-exporter-otlp-proto-grpc-1.25.0 opentelemetry-exporter-otlp-proto-http-1.25.0 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-asgi-0.46b0 opentelemetry-instrumentation-fastapi-0.46b0 opentelemetry-proto-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 opentelemetry-util-http-0.46b0 orjson-3.10.6 overrides-7.7.0 packaging-23.2 posthog-3.5.0 pyjwt-2.8.0 pypdf-4.2.0 pypika-0.48.9 python-dotenv-1.0.1 python-engineio-4.9.1 python-multipart-0.0.9 python-socketio-5.11.3 sentence-transformers-3.0.1 simple-websocket-1.0.0 starlette-0.37.2 striprtf-0.0.26 syncer-2.0.3 text-generation-0.7.0 tiktoken-0.7.0 typing-inspect-0.9.0 uptrace-1.24.0 uvicorn-0.25.0 uvloop-0.19.0 watchfiles-0.20.0 websockets-12.0 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4aiexSl6YLep",
        "outputId": "af98ea8e-01f8-4803-b70e-6edad6582d03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "IBc0ChKTm-Os",
        "outputId": "75bf3933-da2e-40fc-91dd-63a4f39c8910"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chainlit 1.1.306 requires numpy<2.0,>=1.26; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "b0ec5c9251c642ec8238642de41bce98"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader,VectorStoreIndex,SummaryIndex\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.tools import FunctionTool,QueryEngineTool\n",
        "from llama_index.core.vector_stores import MetadataFilters,FilterCondition\n",
        "from typing import List,Optional"
      ],
      "metadata": {
        "id": "mCKVMjvuSGQb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define embedding model and LLM\n"
      ],
      "metadata": {
        "id": "i_DZGD2HeIIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "#from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "#from llama_index.core import Settings\n",
        "from llama_index.llms.mistralai import MistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "class HuggingFaceEmbedding:\n",
        "    def __init__(self, model_name: str):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def get_text_embedding(self, text: str) -> torch.Tensor:\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
        "\n",
        "    def get_embeddings(self, texts: list[str], batch_size: int = 16) -> list[torch.Tensor]:\n",
        "        embeddings = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            inputs = self.tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "            embeddings.extend(batch_embeddings.cpu())\n",
        "        return embeddings\n",
        "\n",
        "embed_model =  HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "llm = MistralAI(model = 'mistral-large-latest', api_key = \"H1rVHKqTjonaHi4vOZ8jozgwtrhRiSjZ\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OuUXOV7vT2d3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read file"
      ],
      "metadata": {
        "id": "L_mNYSyYeNAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(input_files=['Harry Potter The Complete Collection.pdf']).load_data(num_workers=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMdSKjJ1XkZC",
        "outputId": "07cfcaa5-c3d3-4025-c5b0-3e8724ecbae5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/llama_index/core/readers/file/base.py:663: UserWarning: Specified num_workers exceed number of CPUs in the system. Setting `num_workers` down to the maximum CPU count.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chunk the document"
      ],
      "metadata": {
        "id": "pZLG3U8nePUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = SentenceSplitter(chunk_size=1024,chunk_overlap=100)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "print(f\"Length of nodes : {len(nodes)}\")\n",
        "print(f\"get the content for node 0 :{nodes[0].get_content(metadata_mode='all')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBMuh90rdObS",
        "outputId": "e3acb498-81fa-4474-d91e-912b182d3a41"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of nodes : 3623\n",
            "get the content for node 0 :page_label: 1\n",
            "file_name: Harry Potter The Complete Collection.pdf\n",
            "file_path: Harry Potter The Complete Collection.pdf\n",
            "file_type: application/pdf\n",
            "file_size: 17338552\n",
            "creation_date: 2024-07-09\n",
            "last_modified_date: 2024-07-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define relevant contexts\n"
      ],
      "metadata": {
        "id": "eshwoUAFJNjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import ServiceContext, StorageContext\n",
        "\n",
        "service_context = ServiceContext.from_defaults(llm = llm,\n",
        "                                               embed_model = embed_model)\n",
        "                                               #callback_manager = CallbackManager([cl.LlamaIndexCallbackHandler()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHu-oPl7JMk8",
        "outputId": "28c11a45-79ba-4ecc-d303-60b9ac0ee4f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-fb749a54b22e>:3: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(llm = llm,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Alternative 1: Black-box retriever"
      ],
      "metadata": {
        "id": "C6XKMf8aeZOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Vector Store Index"
      ],
      "metadata": {
        "id": "qs_amRC-rdGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = VectorStoreIndex.from_documents(documents,\n",
        "                                               #storage_context=storage_context\n",
        "                                               service_context=service_context,\n",
        "                                               node_parser=nodes,\n",
        "                                               show_progress=True)\n",
        "\n",
        "vector_index.storage_context.persist(persist_dir=\"./storage\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402,
          "referenced_widgets": [
            "ba608f52d40c484c9a2b2f85205ac481",
            "9bc7a991972443f885eab29d8544e949",
            "c3f9667c49c9431d9569ff79defdcd21",
            "491c0e45cab64151a74510cc84a5995b",
            "e2ca414392164e2c8ca8896aefafe451",
            "2997bb41a23f4a3bb51e53f02892414a",
            "04beb3252a714a87b0a05b96ad4c858f",
            "219a445666ba459bb941ea48131f47d4",
            "b2df28fcb27e40a299a7516a5dd9596d",
            "e1ce7c92a31d498da9cda110e5f1d666",
            "dbf00f7a28dd4ea8a3d4e74765ecd4dc"
          ]
        },
        "id": "gcktgmrgebDz",
        "outputId": "c330411c-3bc6-4932-8e15-e437849f18bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/3623 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba608f52d40c484c9a2b2f85205ac481"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'HuggingFaceEmbedding' object has no attribute 'get_text_embedding_batch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f0c27a87c7c4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m vector_index = VectorStoreIndex.from_documents(documents,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                \u001b[0;31m#storage_context=storage_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                \u001b[0mservice_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                \u001b[0mnode_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                show_progress=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/base.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, service_context, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             )\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             return cls(\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mstorage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/vector_store/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insert_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minsert_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mindex_struct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_struct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex_struct\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 index_struct = self.build_index_from_nodes(\n\u001b[0m\u001b[1;32m     95\u001b[0m                     \u001b[0mnodes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/vector_store/base.py\u001b[0m in \u001b[0;36mbuild_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m             )\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_index_from_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minsert_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseNode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minsert_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/vector_store/base.py\u001b[0m in \u001b[0;36m_build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mrun_async_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             self._add_nodes_to_index(\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mindex_struct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/vector_store/base.py\u001b[0m in \u001b[0;36m_add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnodes_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insert_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mnodes_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_node_with_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mnew_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vector_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minsert_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/vector_store/base.py\u001b[0m in \u001b[0;36m_get_node_with_embedding\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m         id_to_embed_map = embed_nodes(\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/utils.py\u001b[0m in \u001b[0;36membed_nodes\u001b[0;34m(nodes, embed_model, show_progress)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mid_to_embed_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     new_embeddings = embed_model.get_text_embedding_batch(\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mtexts_to_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'HuggingFaceEmbedding' object has no attribute 'get_text_embedding_batch'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create query engine"
      ],
      "metadata": {
        "id": "Ep8WYMiHruiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = vector_index.as_query_engine(similarity_top_k = 3,\n",
        "                                            service_context=service_context)\n",
        "\n",
        "response = query_engine.query(\"Who is the main character?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D--KhSivJ4qr",
        "outputId": "ba2b02b7-0e30-4134-9f93-889047a1cb2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main character is a boy named Harry Potter, although he is not directly mentioned in the provided context. The context refers to the Dursleys, who have a secret related to the Potters, and it is mentioned that the Potters have a small son. The file path \"Harry Potter The Complete Collection.pdf\" also suggests that the main character is Harry Potter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deploy the engine on Chainlit"
      ],
      "metadata": {
        "id": "A2lqSIUBNjNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from llama_index.core.callbacks.base import CallbackManager\n",
        "from llama_index.core import load_index_from_storage, StorageContext, ServiceContext\n",
        "from llama_index.llms.mistralai import MistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "import chainlit as cl\n",
        "\n",
        "@cl.on_chat_start\n",
        "async def start_chat():\n",
        "  embed_model = MistralAIEmbedding(model_name=\"mistral-embed\", api_key=\"H1rVHKqTjonaHi4vOZ8jozgwtrhRiSjZ\")\n",
        "\n",
        "  llm = MistralAI(model = 'mistral-large-latest', api_key = \"H1rVHKqTjonaHi4vOZ8jozgwtrhRiSjZ\")\n",
        "  storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "\n",
        "  service_context = ServiceContext.from_defaults(llm = llm,\n",
        "                                                 embed_model = embed_model,\n",
        "                                                 callback_manager = CallbackManager([cl.LlamaIndexCallbackHandler()]))\n",
        "\n",
        "  index = load_index_from_storage(\n",
        "      service_context = service_context,\n",
        "      storage_context = storage_context,\n",
        "  )\n",
        "\n",
        "  query_engine = index.as_query_engine(similarity_top_k = 3,\n",
        "                                       service_context = service_context)\n",
        "\n",
        "  cl.user_session.set(\"chatbot\", query_engine)\n",
        "\n",
        "  await cl.Message(\n",
        "      author=\"Assistant\", content=\"Hello! What do you want to know about Harry Potter?\"\n",
        "  ).send()\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: cl.Message):\n",
        "  query_engine = cl.user_session.get(\"chatbot\")\n",
        "\n",
        "  response = await cl.make_async(query_engine.query)(message.content)\n",
        "\n",
        "  response_message = cl.Message(content=\"\", author=\"Assistant\")\n",
        "\n",
        "  for token in response.response:\n",
        "    await response_message.stream_token(token=token)\n",
        "\n",
        "  await response_message.send()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvP-iE54NFv7",
        "outputId": "b15470f6-9091-4035-da19-ba29b7312ad6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fuser -n tcp -k 8000\n",
        "\n",
        "\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-E50xSnmQ7NS",
        "outputId": "20a8641b-5f69-4883-c0b5-a02b5f867173"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://z9ohyrwfn0d-496ff2e9c6d22116-8000-colab.googleusercontent.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chainlit run app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "poosod0BRZvT",
        "outputId": "80cb34d9-a4ab-44f7-b577-005f21032673"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1134, in _common_field_schema\n",
            "    schema = self._apply_annotations(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1890, in _apply_annotations\n",
            "    schema = get_inner_schema(source_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_schema_generation_shared.py\", line 83, in __call__\n",
            "    schema = self._handler(source_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1871, in inner_handler\n",
            "    schema = self._generate_schema_inner(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 789, in _generate_schema_inner\n",
            "    return self.match_type(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 871, in match_type\n",
            "    return self._match_generic_type(obj, origin)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 895, in _match_generic_type\n",
            "    return self._union_schema(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1200, in _union_schema\n",
            "    args = self._get_args_resolving_forward_refs(union_type, required=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 743, in _get_args_resolving_forward_refs\n",
            "    def _get_args_resolving_forward_refs(self, obj: Any, required: bool = False) -> tuple[Any, ...] | None:\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/chainlit\", line 5, in <module>\n",
            "    from chainlit.cli import cli\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chainlit/__init__.py\", line 10, in <module>\n",
            "    from fastapi import Request, Response\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/__init__.py\", line 7, in <module>\n",
            "    from .applications import FastAPI as FastAPI\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 16, in <module>\n",
            "    from fastapi import routing\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 22, in <module>\n",
            "    from fastapi import params\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/params.py\", line 5, in <module>\n",
            "    from fastapi.openapi.models import Example\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/openapi/models.py\", line 299, in <module>\n",
            "    class Operation(BaseModelWithConfig):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_model_construction.py\", line 205, in __new__\n",
            "    complete_model_class(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_model_construction.py\", line 534, in complete_model_class\n",
            "    schema = cls.__get_pydantic_core_schema__(cls, handler)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/main.py\", line 642, in __get_pydantic_core_schema__\n",
            "    return handler(source)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_schema_generation_shared.py\", line 83, in __call__\n",
            "    schema = self._handler(source_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 512, in generate_schema\n",
            "    schema = self._generate_schema_inner(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 784, in _generate_schema_inner\n",
            "    return self._model_schema(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 591, in _model_schema\n",
            "    {k: self._generate_md_field_schema(k, v, decorators) for k, v in fields.items()},\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 591, in <dictcomp>\n",
            "    {k: self._generate_md_field_schema(k, v, decorators) for k, v in fields.items()},\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 947, in _generate_md_field_schema\n",
            "    common_field = self._common_field_schema(name, field_info, decorators)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1134, in _common_field_schema\n",
            "    schema = self._apply_annotations(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1890, in _apply_annotations\n",
            "    schema = get_inner_schema(source_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_schema_generation_shared.py\", line 83, in __call__\n",
            "    schema = self._handler(source_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1871, in inner_handler\n",
            "    schema = self._generate_schema_inner(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 789, in _generate_schema_inner\n",
            "    return self.match_type(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 871, in match_type\n",
            "    return self._match_generic_type(obj, origin)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 895, in _match_generic_type\n",
            "    return self._union_schema(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1207, in _union_schema\n",
            "    choices.append(self.generate_schema(arg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 512, in generate_schema\n",
            "    schema = self._generate_schema_inner(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 789, in _generate_schema_inner\n",
            "    return self.match_type(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 867, in match_type\n",
            "    return self._apply_annotations(source_type, annotations)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1890, in _apply_annotations\n",
            "    schema = get_inner_schema(source_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_schema_generation_shared.py\", line 83, in __call__\n",
            "    schema = self._handler(source_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1972, in new_handler\n",
            "    schema = metadata_get_schema(source, get_inner_schema)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_std_types_schema.py\", line 316, in __get_pydantic_core_schema__\n",
            "    items_schema = handler.generate_schema(self.item_source_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_schema_generation_shared.py\", line 97, in generate_schema\n",
            "    return self._generate_schema.generate_schema(source_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 512, in generate_schema\n",
            "    schema = self._generate_schema_inner(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 789, in _generate_schema_inner\n",
            "    return self.match_type(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 871, in match_type\n",
            "    return self._match_generic_type(obj, origin)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 895, in _match_generic_type\n",
            "    return self._union_schema(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1207, in _union_schema\n",
            "    choices.append(self.generate_schema(arg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 507, in generate_schema\n",
            "    from_property = self._generate_schema_from_property(obj, obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 679, in _generate_schema_from_property\n",
            "    schema = get_schema(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/main.py\", line 642, in __get_pydantic_core_schema__\n",
            "    return handler(source)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_schema_generation_shared.py\", line 83, in __call__\n",
            "    schema = self._handler(source_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 784, in _generate_schema_inner\n",
            "    return self._model_schema(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 591, in _model_schema\n",
            "    {k: self._generate_md_field_schema(k, v, decorators) for k, v in fields.items()},\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 591, in <dictcomp>\n",
            "    {k: self._generate_md_field_schema(k, v, decorators) for k, v in fields.items()},\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 947, in _generate_md_field_schema\n",
            "    common_field = self._common_field_schema(name, field_info, decorators)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1134, in _common_field_schema\n",
            "    schema = self._apply_annotations(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1890, in _apply_annotations\n",
            "    schema = get_inner_schema(source_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_schema_generation_shared.py\", line 83, in __call__\n",
            "    schema = self._handler(source_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1871, in inner_handler\n",
            "    schema = self._generate_schema_inner(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 789, in _generate_schema_inner\n",
            "    return self.match_type(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 871, in match_type\n",
            "    return self._match_generic_type(obj, origin)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 895, in _match_generic_type\n",
            "    return self._union_schema(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1207, in _union_schema\n",
            "    choices.append(self.generate_schema(arg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 507, in generate_schema\n",
            "    from_property = self._generate_schema_from_property(obj, obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 679, in _generate_schema_from_property\n",
            "    schema = get_schema(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/main.py\", line 642, in __get_pydantic_core_schema__\n",
            "    return handler(source)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_schema_generation_shared.py\", line 83, in __call__\n",
            "    schema = self._handler(source_type)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 784, in _generate_schema_inner\n",
            "    return self._model_schema(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 591, in _model_schema\n",
            "    {k: self._generate_md_field_schema(k, v, decorators) for k, v in fields.items()},\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 591, in <dictcomp>\n",
            "    {k: self._generate_md_field_schema(k, v, decorators) for k, v in fields.items()},\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 947, in _generate_md_field_schema\n",
            "    common_field = self._common_field_schema(name, field_info, decorators)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_generate_schema.py\", line 1134, in _common_field_schema\n",
            "    schema = self._apply_annotations(\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tLqDIt5kRbSR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Alternative 2: Custom Retriever\n"
      ],
      "metadata": {
        "id": "xD5fDcdwpqYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "cHKvXSp0sIq8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define top-k embedding retriever"
      ],
      "metadata": {
        "id": "twtjw5Dipxtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_embed_retriever(query: str, k: int, embed_model, manual_embed_np, texts):\n",
        "    query_embed = embed_model.get_text_embedding(query)\n",
        "    query_embed_np = query_embed.cpu().numpy().reshape(1, -1)\n",
        "\n",
        "    # Calculate cosine similarity with all the other embeddings\n",
        "    similarities = cosine_similarity(query_embed_np, manual_embed_np).flatten()\n",
        "\n",
        "    # Find the top k indices\n",
        "    top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
        "    top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]\n",
        "\n",
        "    # Retrieve the top k texts\n",
        "    top_k_texts = [texts[i] for i in top_k_indices]\n",
        "\n",
        "    return top_k_texts"
      ],
      "metadata": {
        "id": "ij1WFq4Op3-F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create embeddings for each node"
      ],
      "metadata": {
        "id": "B73KLijKqwT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLKNodoX1r3O",
        "outputId": "e8b42b37-79e0-42d5-f71b-e30424f4a0f3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "node_embeddings = []\n",
        "node_texts = [node.get_text() for node in nodes]\n",
        "for text in tqdm(node_texts):\n",
        "    embed = embed_model.get_text_embedding(text)\n",
        "    node_embeddings.append(embed.cpu().numpy())\n",
        "\n",
        "node_embeddings_np = np.array(node_embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJJ_X-twqwAP",
        "outputId": "a43d1648-da75-412d-8b8e-cdebad0b1346"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3623/3623 [30:23<00:00,  1.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save embeddings for later use"
      ],
      "metadata": {
        "id": "lKtUVOApq6Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('node_embeddings.npy', node_embeddings_np)\n",
        "with open('node_texts.json', 'w') as f:\n",
        "    json.dump(node_texts, f)"
      ],
      "metadata": {
        "id": "Nyros9FPq89t"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Find the relative passages to a query with the retriever\n",
        "\n"
      ],
      "metadata": {
        "id": "dzPXh-EGr3TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_query_engine(query, k=3):\n",
        "    results = top_k_embed_retriever(query, k, embed_model, node_embeddings_np, node_texts)\n",
        "    return results"
      ],
      "metadata": {
        "id": "2snxAsjMr8Tb"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who is the main character?\"\n",
        "retrieved = custom_query_engine(query)\n",
        "\n",
        "combined_text = \". \".join(retrieved)"
      ],
      "metadata": {
        "id": "mDIcAQopDEQ_"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define Prompt Format"
      ],
      "metadata": {
        "id": "ek-7N-Z3sBRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{combined_text}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {query}\n",
        "Answer:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "L7c0X_DmsC49"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deploy Mistral Client"
      ],
      "metadata": {
        "id": "I01Fg3oBDXli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai.models.chat_completion import ChatMessage\n",
        "from mistralai.client import MistralClient\n",
        "\n",
        "model = 'mistral-large-latest'\n",
        "client = MistralClient(api_key=\"H1rVHKqTjonaHi4vOZ8jozgwtrhRiSjZ\")\n",
        "\n",
        "def generate_response(prompt):\n",
        "  messages = [\n",
        "      ChatMessage(role=\"user\", content=prompt)\n",
        "      ]\n",
        "  chat_response = client.chat(\n",
        "      model=model,\n",
        "      messages=messages\n",
        "  )\n",
        "\n",
        "  return (chat_response.choices[0].message.content)\n",
        "\n",
        "print(generate_response(prompt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRGlx9-AB-gc",
        "outputId": "b5ef06d2-c5e0-404f-f42e-87c091adf302"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main character in the provided text appears to be Harry. He is the one asking questions, interacting with various characters, and expressing his thoughts and feelings. The text also mentions other characters such as Hermione, Bathilda, Malfoy, Snape, Cedric, Cho, Fleur, and Krum, but Harry seems to be the central figure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deploy model on chainlit using mistral client"
      ],
      "metadata": {
        "id": "Z2YuYr-gF3q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_custom.py\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "from mistralai.client import MistralClient\n",
        "import chainlit as cl\n",
        "import json\n",
        "#from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "#from llama_index.core import Settings\n",
        "from llama_index.llms.mistralai import MistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "class HuggingFaceEmbedding:\n",
        "    def __init__(self, model_name: str):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def get_text_embedding(self, text: str) -> torch.Tensor:\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
        "\n",
        "    def get_embeddings(self, texts: list[str], batch_size: int = 16) -> list[torch.Tensor]:\n",
        "        embeddings = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            inputs = self.tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "            embeddings.extend(batch_embeddings.cpu())\n",
        "        return embeddings\n",
        "\n",
        "embed_model =  HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "model = 'mistral-large-latest'\n",
        "client = MistralClient(api_key=\"H1rVHKqTjonaHi4vOZ8jozgwtrhRiSjZ\")\n",
        "\n",
        "def top_k_embed_retriever(query: str, k: int, embed_model, manual_embed_np, texts):\n",
        "    query_embed = embed_model.get_text_embedding(query)\n",
        "    query_embed_np = query_embed.cpu().numpy().reshape(1, -1)\n",
        "\n",
        "    # Calculate cosine similarity with all the other embeddings\n",
        "    similarities = cosine_similarity(query_embed_np, manual_embed_np).flatten()\n",
        "\n",
        "    # Find the top k indices\n",
        "    top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
        "    top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]\n",
        "\n",
        "    # Retrieve the top k texts\n",
        "    top_k_texts = [texts[i] for i in top_k_indices]\n",
        "\n",
        "    return top_k_texts\n",
        "\n",
        "def custom_query_engine(query, k=3):\n",
        "    node_embeddings_np = np.load('node_embeddings.npy')\n",
        "    with open('node_texts.json', 'r') as f:\n",
        "        node_texts = json.load(f)\n",
        "    results = top_k_embed_retriever(query, k, embed_model, node_embeddings_np, node_texts)\n",
        "    return results\n",
        "\n",
        "def find_text(query):\n",
        "  retrieved = custom_query_engine(query)\n",
        "\n",
        "  combined_text = \". \".join(retrieved)\n",
        "\n",
        "  return combined_text\n",
        "\n",
        "def create_prompt(combined_text, query):\n",
        "  prompt = f\"\"\"\n",
        "    Context information is below.\n",
        "    ---------------------\n",
        "    {combined_text}\n",
        "    ---------------------\n",
        "    Given the context information and not prior knowledge, answer the query.\n",
        "    Query: {query}\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "  return prompt\n",
        "\n",
        "@cl.on_chat_start\n",
        "async def start_chat():\n",
        "  welcome_message = cl.Message(content=\"Starting the chatbot...\")\n",
        "  await welcome_message.send()\n",
        "  welcome_message.content = (\n",
        "      \"Hi, what do you want to know about Harry Potter?\"\n",
        "  )\n",
        "  await welcome_message.update()\n",
        "    #cl.user_session.set(\"chain\", chatbot)\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: cl.Message):\n",
        "  #chatbot = cl.user_session.get(\"chain\")\n",
        "\n",
        "  combined_text = find_text(message.content)\n",
        "  prompt = create_prompt(combined_text, message.content)\n",
        "\n",
        "  messages = [\n",
        "    ChatMessage(role=\"user\",\n",
        "                content=prompt)\n",
        "  ]\n",
        "  chat_response = client.chat(\n",
        "    model=model,\n",
        "    messages=messages\n",
        "  )\n",
        "\n",
        "  await cl.Message(chat_response.choices[0].message.content).send()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GKU2kslF7GJ",
        "outputId": "dec25b5b-c44e-403a-8916-88f96516fb6a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_custom.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fuser -n tcp -k 8000\n",
        "\n",
        "\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bFA95U3MMfG2",
        "outputId": "a7c54706-c07a-4e12-fb8a-35d535c4480a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://z9ohyrwfn0d-496ff2e9c6d22116-8000-colab.googleusercontent.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chainlit run app_custom.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG6e3IvoMn5P",
        "outputId": "9acf71d9-6881-4747-8fc8-9058e54920bd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-09 16:00:21 - Your app is available at http://localhost:8000\n",
            "2024-07-09 16:00:28 - Translated markdown file for en-US not found. Defaulting to chainlit.md.\n",
            "2024-07-09 16:00:40 - HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kjM3wihzOUsb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}